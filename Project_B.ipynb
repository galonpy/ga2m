{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A variable's single variable graph across each time subperiod\n",
    "#compared to its heatmap with a whole panel dataset against the variable of time\n",
    "\n",
    "#Dataset: panel data\n",
    "\n",
    "#Value to see if the result is consistent\n",
    "\n",
    "#how does the Sum of the predictor coefficients relate as a statistic to the observed probablity in the trained population\n",
    "#if you split up into subpopulations of higher risk does it associate monotonically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.mlab as mlab\n",
    "%matplotlib inline\n",
    "from interpret import show\n",
    "from interpret.data import ClassHistogram\n",
    "from interpret.glassbox import ExplainableBoostingClassifier, LogisticRegression, ClassificationTree, DecisionListClassifier\n",
    "from interpret.perf import ROC\n",
    "from interpret.glassbox import LogisticRegression, ClassificationTree\n",
    "#without the class structure single shot each time\n",
    "from interpret import show\n",
    "from interpret.data import ClassHistogram\n",
    "from interpret.glassbox import ExplainableBoostingClassifier, LogisticRegression, ClassificationTree, DecisionListClassifier\n",
    "from interpret.glassbox import LogisticRegression, ClassificationTree\n",
    "import collections\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ExplainableBoostingClassifier in module interpret.glassbox.ebm.ebm:\n",
      "\n",
      "class ExplainableBoostingClassifier(BaseEBM, sklearn.base.ClassifierMixin, interpret.api.base.ExplainerMixin)\n",
      " |  ExplainableBoostingClassifier(feature_names=None, feature_types=None, schema=None, n_estimators=16, holdout_size=0.15, scoring=None, main_attr='all', interactions=0, holdout_split=0.15, data_n_episodes=2000, early_stopping_tolerance=1e-05, early_stopping_run_length=50, feature_step_n_inner_bags=0, learning_rate=0.01, training_step_episodes=1, max_tree_splits=2, min_cases_for_splits=2, n_jobs=-2, random_state=42, binning_strategy='uniform')\n",
      " |  \n",
      " |  Client facing SK EBM.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ExplainableBoostingClassifier\n",
      " |      BaseEBM\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      interpret.api.base.ExplainerMixin\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, feature_names=None, feature_types=None, schema=None, n_estimators=16, holdout_size=0.15, scoring=None, main_attr='all', interactions=0, holdout_split=0.15, data_n_episodes=2000, early_stopping_tolerance=1e-05, early_stopping_run_length=50, feature_step_n_inner_bags=0, learning_rate=0.01, training_step_episodes=1, max_tree_splits=2, min_cases_for_splits=2, n_jobs=-2, random_state=42, binning_strategy='uniform')\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      # TODO: Throw ValueError like scikit for 1d instead of 2d arrays\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  available_explanations = ['global', 'local']\n",
      " |  \n",
      " |  explainer_type = 'model'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseEBM:\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |  \n",
      " |  explain_global(self, name=None)\n",
      " |  \n",
      " |  explain_local(self, X, y=None, name=None)\n",
      " |  \n",
      " |  fit(self, X, y)\n",
      " |      # NOTE: Consider refactoring later.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ExplainableBoostingClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan missing values by column\n",
      "education     105\n",
      "cigsPerDay     29\n",
      "BPMeds         53\n",
      "totChol        50\n",
      "BMI            19\n",
      "heartRate       1\n",
      "glucose       388\n",
      "dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Series([], dtype: int64)\n",
      "Series([], dtype: int64)\n",
      "Series([], dtype: int64)\n",
      "Missing values remaining\n"
     ]
    }
   ],
   "source": [
    "#Upload Data and Data Cleanup\n",
    "\n",
    "df = pd.read_csv(\"/Users/gabrielalon/Desktop/heartdisease.csv\")\n",
    "df.columns = [\n",
    "    \"male\",\"age\",\"education\",\"currentSmoker\",\"cigsPerDay\",\"BPMeds\",\"prevalentStroke\",\"prevalentHyp\",\"diabetes\",\"totChol\",\"sysBP\",\"diaBP\",\"BMI\",\"heartRate\",\"glucose\",\"TenYearCHD\"\n",
    "]\n",
    "print(\"Nan missing values by column\")\n",
    "#Number of Nans by column\n",
    "missing_val_count_by_column = (df.isnull().sum())\n",
    "print(missing_val_count_by_column[missing_val_count_by_column > 0])\n",
    "print(type(df))\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "missing_val_count_by_column = (df.isnull().sum())\n",
    "print(missing_val_count_by_column[missing_val_count_by_column > 0])\n",
    "df.head()\n",
    "missing_val_count_by_column = (df.isnull().sum())\n",
    "print(missing_val_count_by_column[missing_val_count_by_column > 0])\n",
    "#dropping NaN rows\n",
    "\n",
    "print(missing_val_count_by_column[missing_val_count_by_column > 0])\n",
    "df.dropna(axis=0,inplace=True)\n",
    "\n",
    "missing_val_count_by_column = (df.isnull().sum())\n",
    "print(\"Missing values remaining\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples\n",
      "4240\n",
      "Number of samples in each dataset now\n",
      "424\n",
      "424\n"
     ]
    }
   ],
   "source": [
    "#Split my data into 10 datasets\n",
    "\n",
    "#df = pd.read_csv(\"/Users/gabrielalon/Desktop/heart.csv\")\n",
    "#df.columns = [\"Age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\", \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"target\"]\n",
    "#df.fillna(df.mean(), inplace=True)\n",
    "#missing_val_count_by_column = (df.isnull().sum())\n",
    "#print(missing_val_count_by_column[missing_val_count_by_column > 0])\n",
    "#df.head()\n",
    "#Total = sum(df.TenYearCHD)\n",
    "#print (Total)\n",
    "print(\"Number of samples\")\n",
    "print(len(df))\n",
    "df1 = df.sample(frac=0.1)\n",
    "df2 = df.sample(frac=0.1)\n",
    "df3 = df.sample(frac=0.1)\n",
    "df4 = df.sample(frac=0.1)\n",
    "df5 = df.sample(frac=0.1)\n",
    "df6 = df.sample(frac=0.1)\n",
    "df7 = df.sample(frac=0.1)\n",
    "df8 = df.sample(frac=0.1)\n",
    "df9 = df.sample(frac=0.1)\n",
    "df10 = df.sample(frac=0.1)\n",
    "print(\"Number of samples in each dataset now\")\n",
    "print(len(df1))\n",
    "print(len(df2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need a mapping between input df and time index\n",
    "#map_time = {'df0':0, 'df1':1, 'df2':2, 'df3':3, 'df4':4, 'df5':5, 'df6':6, 'df7':7, 'df8':8, 'df9':9, 'df10':10}\n",
    "\n",
    "df1_copy = df1\n",
    "num1 = [1]*len(df1_copy)\n",
    "df2_copy = df2\n",
    "num2 = [2]*len(df2_copy)\n",
    "df3_copy = df3\n",
    "num3 = [3]*len(df3_copy)\n",
    "df4_copy = df4\n",
    "num4 = [4]*len(df4_copy)\n",
    "df5_copy = df5\n",
    "num5 = [5]*len(df5_copy)\n",
    "df6_copy = df6\n",
    "num6 = [6]*len(df6_copy)\n",
    "df7_copy = df7\n",
    "num7 = [7]*len(df7_copy)\n",
    "df8_copy = df8\n",
    "num8 = [8]*len(df8_copy)\n",
    "df9_copy = df9\n",
    "num9 = [9]*len(df9_copy)\n",
    "df10_copy = df10\n",
    "num10 = [10]*len(df10_copy)\n",
    "\n",
    "\"\"\"\n",
    "df1['year'] = (num1)\n",
    "df2['year'] = (num2)\n",
    "df3['year'] = (num3)\n",
    "df4['year'] = (num4)\n",
    "df5['year'] = (num5)\n",
    "df6['year'] = (num6)\n",
    "df7['year'] = (num7)\n",
    "df8['year'] = (num8)\n",
    "df9['year'] = (num9)\n",
    "df10['year'] = (num10)\n",
    "\"\"\"\n",
    "\n",
    "df1.insert(1,'panel_year',num1)\n",
    "df2.insert(2,'panel_year',num2)\n",
    "df3.insert(3,'panel_year',num3)\n",
    "df4.insert(4,'panel_year',num4)\n",
    "df5.insert(5,'panel_year',num5)\n",
    "df6.insert(6,'panel_year',num6)\n",
    "df7.insert(7,'panel_year',num7)\n",
    "df8.insert(8,'panel_year',num8)\n",
    "df9.insert(9,'panel_year',num9)\n",
    "df10.insert(10,'panel_year',num10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a fake trend across time to test the model \n",
    "#print(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Manipulation Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['sysBP'] = 1.05*df1['sysBP']\n",
    "df2['sysBP'] = 1.1*df2['sysBP']\n",
    "df3['sysBP'] = 1.15*df3['sysBP']\n",
    "df4['sysBP'] = 1.2*df4['sysBP']\n",
    "df5['sysBP'] = 1.25*df5['sysBP']\n",
    "df6['sysBP'] = 1.3*df6['sysBP']\n",
    "df7['sysBP'] = 1.35*df7['sysBP']\n",
    "df8['sysBP'] = 1.4*df8['sysBP']\n",
    "df9['sysBP'] = 1.45*df9['sysBP']\n",
    "df10['sysBP'] = 10*df10['sysBP']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 51\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TenYearCHD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2638</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1913</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TenYearCHD\n",
       "2638           0\n",
       "1913           0\n",
       "254            0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = df2.iloc[0,1]\n",
    "b = df2.iloc[1,1]\n",
    "print(a,b)\n",
    "#row, column\n",
    "df2.iloc[0:3,16:17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Or manipulate the dependent variables of only one outcome state at a time for a clear signal.\n",
    "#Here im manipulating all rows where the patient got the disease by increasing the number of cigarretes all of them smoked\n",
    "#by 30%\n",
    "for i in range(len(df1)):\n",
    "    if df2.iloc[i,16] == 1:\n",
    "        df2.iloc[i,5] = df2.iloc[i,5]*5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "frames = [df1,df2,df3,df4,df5,df6,df7,df8,df9,df10]\n",
    "panel_data = pd.concat(frames, sort=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitting(df):\n",
    "    df = df\n",
    "    train_cols = df.columns[0:-1]\n",
    "    label = df.columns[-1]\n",
    "    X = df[train_cols]\n",
    "    y = df[label] \n",
    "    #y = df[label].apply(lambda x: 0 if x == \" <=50K\" else 1) #Turning response into 0 and 1\n",
    "    seed = 1\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=seed)\n",
    "    return [X_train, y_train, X_test, y_test, X, y]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ga2m:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.globals = 0\n",
    "        \n",
    "    def splitting(df):\n",
    "            df = df\n",
    "            train_cols = df.columns[0:-1]\n",
    "            label = df.columns[-1]\n",
    "            X = df[train_cols]\n",
    "            y = df[label] \n",
    "            #y = df[label].apply(lambda x: 0 if x == \" <=50K\" else 1) #Turning response into 0 and 1\n",
    "            seed = 1\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=seed)\n",
    "            return [X_train, y_train, X_test, y_test, X, y]\n",
    "        \n",
    "    splitting = splitting(df1)\n",
    "    X_train = splitting[0] \n",
    "    y_train = splitting[1]\n",
    "    X_test = splitting[2] \n",
    "    y_test = splitting[3]\n",
    "    X = splitting[4]\n",
    "    y = splitting[5]\n",
    "\n",
    "        \n",
    "    def plot(self):\n",
    "        df = self.df\n",
    "        from interpret import show\n",
    "        from interpret.data import ClassHistogram\n",
    "        from interpret.glassbox import ExplainableBoostingClassifier, LogisticRegression, ClassificationTree, DecisionListClassifier\n",
    "        from interpret.perf import ROC\n",
    "        from interpret.glassbox import LogisticRegression, ClassificationTree\n",
    "        train_cols = df.columns[0:-1]\n",
    "        label = df.columns[-1]\n",
    "        X = df[train_cols]\n",
    "        y = df[label] \n",
    "        #y = df[label].apply(lambda x: 0 if x == \" <=50K\" else 1) #Turning response into 0 and 1\n",
    "\n",
    "        seed = 1\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=seed)\n",
    "\n",
    "        hist = ClassHistogram().explain_data(X_train, y_train, name = 'Train Data')\n",
    "        #show(hist)\n",
    "\n",
    "        #ebm = ExplainableBoostingClassifier(random_state=seed)\n",
    "       # ebm = ExplainableBoostingClassifier(interactions=[[0, 1], [2, 3]])\n",
    "        ebm = ExplainableBoostingClassifier(interactions=[[1, 5]])\n",
    "     #   ebm = ExplainableBoostingClassifier(interactions=1, random_state=seed)\n",
    "        ebm.fit(X_train, y_train)   #Works on dataframes and numpy arrays\n",
    "\n",
    "        ebm_global = ebm.explain_global(name='EBM')\n",
    "        self.globals = ebm_global\n",
    "       # return ebm_global\n",
    "        graph_data = ebm_global.data(-1)\n",
    "        return(graph_data)\n",
    "       # show(ebm_global)\n",
    "        ebm_local = ebm.explain_local(X_test[:5], y_test[:5], name='EBM')\n",
    "        #show(ebm_local)\n",
    "        ebm_perf = ROC(ebm.predict_proba).explain_perf(X_test, y_test, name='EBM')\n",
    "      #  show(ebm_perf)\n",
    "\n",
    "        # We have to transform categorical variables to use Logistic Regression and Decision Tree\n",
    "        X_enc = pd.get_dummies(X, prefix_sep='.')\n",
    "        feature_names = list(X_enc.columns)\n",
    "        X_train_enc, X_test_enc, y_train, y_test = train_test_split(X_enc, y, test_size=0.20, random_state=seed)\n",
    "\n",
    "        lr = LogisticRegression(random_state=seed, feature_names=feature_names, penalty='l1')\n",
    "        lr.fit(X_train_enc, y_train)\n",
    "        help(lr)\n",
    "\n",
    "        tree = ClassificationTree()\n",
    "        tree.fit(X_train_enc, y_train)\n",
    "\n",
    "        lr_perf = ROC(lr.predict_proba).explain_perf(X_test_enc, y_test, name='Logistic Regression')\n",
    "        tree_perf = ROC(tree.predict_proba).explain_perf(X_test_enc, y_test, name='Classification Tree')\n",
    "        print(type(tree_perf))\n",
    "      #  show(lr_perf)\n",
    "       # show(tree_perf)\n",
    "       # show(ebm_perf)\n",
    "\n",
    "        lr_global = lr.explain_global(name='LR')\n",
    "        tree_global = tree.explain_global(name='Tree')\n",
    "\n",
    "       # show(lr_global)\n",
    "      #  show(tree_global)\n",
    "       # show(ebm_global)\n",
    "     #   show([hist, lr_global, lr_perf, tree_global, tree_perf, ebm_global, ebm_perf], share_tables=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = ga2m(df1)\n",
    "result1 = p1.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__contains__', '__delattr__', '__delitem__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'clear', 'copy', 'fromkeys', 'get', 'items', 'keys', 'pop', 'popitem', 'setdefault', 'update', 'values']\n"
     ]
    }
   ],
   "source": [
    "#print(result1)\n",
    "print(dir(result1))\n",
    "#{'type': 'pairwise', 'left_names':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run on each time period\n",
    "p0 = ga2m(df)\n",
    "p1 = ga2m(df1)\n",
    "p2 = ga2m(df2)\n",
    "p3 = ga2m(df3)\n",
    "p4 = ga2m(df4)\n",
    "p5 = ga2m(df5)\n",
    "p6 = ga2m(df6)\n",
    "p7 = ga2m(df7)\n",
    "p8 = ga2m(df8)\n",
    "p9 = ga2m(df9)\n",
    "\n",
    "\n",
    "result0 = p0.plot()\n",
    "result1 = p1.plot()\n",
    "result2 = p2.plot()\n",
    "result3 = p3.plot()\n",
    "result4 = p4.plot()\n",
    "result5 = p5.plot()\n",
    "result6 = p6.plot()\n",
    "result7 = p7.plot()\n",
    "result8 = p8.plot()\n",
    "result9 = p9.plot()\n",
    "#print((result1))\n",
    "#resultprime = p3.plot()\n",
    "results = []\n",
    "#all the pairwise heatmaps\n",
    "results = [result1, result2, result3, result4, result5, result6, result7, result8, result9]\n",
    "#print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ebm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-39a9e0735659>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#print(result2['specific'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhelp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mebm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ebm' is not defined"
     ]
    }
   ],
   "source": [
    "#print(result2['specific'])\n",
    "\n",
    "help(ebm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(result0['specific'])\n",
    "#print('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(results)\n",
    "for key, value in result1.items() :\n",
    "    print(key)\n",
    "#left_names right_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mli = (result1['mli'])\n",
    "#print(mli[1]['names'])\n",
    "#print((mli)[0])\n",
    "#MLI is:                                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the Variable for whose 'specific' data will be accessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'overall' is simply one value coefficient sumamary per variable \n",
    "#and doesn't vary for each variable like its actually defined\n",
    "\n",
    "specific = (result0['specific'])\n",
    "#print(specific)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_series = (result0['specific'],result1['specific'],result2['specific'],result3['specific'],result4['specific'],result5['specific'],result6['specific'],result7['specific'],result8['specific'],result9['specific'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(result1['overall'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "print(\"structure for the single variable\")\n",
    "for key, value in specific[1].items() :\n",
    "    print(key)\n",
    "    print(key, type(specific[1][str(key)]))\n",
    "print('\\n')\n",
    "print(\"structure for the pairwise variable\")\n",
    "for key, value in specific[-1].items() :\n",
    "    print(key)\n",
    "    print(key, type(specific[-1][str(key)]))\n",
    "#\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the contents\n",
    "\"\"\"\n",
    "structure for the single variable\n",
    "type\n",
    "type <class 'str'>\n",
    "names\n",
    "names <class 'list'>\n",
    "scores\n",
    "scores <class 'numpy.ndarray'>\n",
    "scores_range\n",
    "scores_range <class 'tuple'>\n",
    "upper_bounds\n",
    "upper_bounds <class 'numpy.ndarray'>\n",
    "lower_bounds\n",
    "lower_bounds <class 'numpy.ndarray'>\n",
    "density\n",
    "density <class 'dict'>\n",
    "\n",
    "\n",
    "structure for the pairwise variable\n",
    "type\n",
    "type <class 'str'>\n",
    "left_names\n",
    "left_names <class 'list'>\n",
    "right_names\n",
    "right_names <class 'list'>\n",
    "scores\n",
    "scores <class 'numpy.ndarray'>\n",
    "scores_range\n",
    "scores_range <class 'tuple'>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validates whether its a single variable or a pairwise graph that i'm accessing\n",
    "#pairwise_index contains the indices of the pairwsie graphs at the tailend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = float('NaN')\n",
    "b = 5\n",
    "c = np.nan\n",
    "try:\n",
    "    if a>b:\n",
    "        print(\"yes\")\n",
    "except:\n",
    "    print(\"no\")\n",
    "try:\n",
    "    if b>c:\n",
    "        (print(\"go\"))\n",
    "except:\n",
    "    print(\"snow\")\n",
    "\n",
    "#the user can input a request to the right of the left of the Nan. The problem is that\n",
    "#this interferes with the binary search algorithm because its close to terminating when you are already comparing\n",
    "#it to the Nan itself. probably best to make a copy of the whole dataframe with the nan deleted then rerun#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[https://stackoverflow.com/questions/2868342/how-to-search-for-closest-value-in-a-lookup-table]\n",
    "import bisect\n",
    "#Returns the closest entry in the sorted list 'sorted' to 'value'. Returns an index rather than the value\n",
    "def find_closest(sorted, value):\n",
    " #   sorted_cp = sorted\n",
    "    sorted_cp = [x for x in sorted if str(x) != 'nan']\n",
    "  #  for i in range(len(sorted_cp)):\n",
    "   #     if sorted[i] == nan:\n",
    "    #        print(sorted[i])\n",
    "     #       del(sorted[i])\n",
    "            \n",
    "    if (value <= sorted_cp[0]):\n",
    "     #    return sorted[0]\n",
    "         return int(0)\n",
    "    if (value >= sorted_cp[-1]):\n",
    "    #    return sorted[-1]\n",
    "        return int(len(sorted_cp)-1)\n",
    "    insertpos = bisect.bisect(sorted_cp, value)\n",
    "    if (abs(sorted_cp[insertpos-1] - value) <= abs(sorted_cp[insertpos] - value)):\n",
    "     #   return sorted_cp[insertpos-1]\n",
    "        return(int(insertpos))-1\n",
    "    else:\n",
    "     #   return sorted_cp[insertpos]\n",
    "        return(int(insertpos))-1\n",
    "    \n",
    "#have to either forcerfully ignore nans and resort to the next closest value and or\n",
    "#identify the source of the nans. Or delete the Nan?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need to find and output sortable range for pairwise heatmap\n",
    "#Need to validate they fixed the problem the software update was for i think its fine\n",
    "#Need to find the mapping of the actual variable name to its index in the graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pairwise uses x,y coordinates aka left and right in the data which the user has to specify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific = (result0['specific'])\n",
    "#where result# above is the dataset # trained on\n",
    "\n",
    "def range_pairwise(specific):\n",
    "    pairwise_indexer = []\n",
    "    for i in range(len(specific)):\n",
    "        for key, value in specific[i].items():\n",
    "            if key == 'type':\n",
    "                if (specific[i]['type']) == 'pairwise':\n",
    "                    #print(i)\n",
    "                    return True \n",
    "\n",
    "pairwise_index = []\n",
    "pairwise_index.append(range_pairwise(specific))\n",
    "\"\"\"\n",
    "def pairwise_bool(specific):\n",
    " #   pairwise_indexer = []\n",
    "    for i in range(len(specific)):\n",
    "        for key, value in specific[i].items():\n",
    "            if key == 'type':\n",
    "                if (specific[i]['type']) == 'pairwise':\n",
    "                    return True \n",
    "                    #print(i)\n",
    "                   # pairwise_indexer.append(i)\n",
    "\"\"\"\n",
    "def pairwise_bool(dependent_variable):\n",
    " #   pairwise_indexer = []=\n",
    "    if (specific[dependent_variable]['type']) == 'pairwise':\n",
    "        return True \n",
    "                    #print(i)\n",
    "                   # pairwise_indexer.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need mapping from dependent variable name to index. currently only accept index\n",
    "\n",
    "def pairwise_procedure(dependent_variable, dependent_variable_value_x,dependent_variable_value_y):\n",
    "    #consider whether the user specifies the dependent variable name instead of the actual index\n",
    "    #perhaps range_pairwise should actual be a boolean for its pairwise\n",
    "    yaxis = specific[dependent_variable]['right_names']\n",
    "    xaxis = specific[dependent_variable]['left_names']\n",
    "    try:\n",
    "        xval  = find_closest(xaxis, dependent_variable_value_x)\n",
    "    except:\n",
    "        print(\"xval broke\")\n",
    "        print(xaxis)\n",
    "\n",
    "    try:\n",
    "        yval = find_closest(yaxis, dependent_variable_value_y)\n",
    "    except:\n",
    "        print(\"yval broke\")\n",
    "        print(yaxis)\n",
    "   # print(xval)\n",
    "  #  print(yval)\n",
    "    return (specific[dependent_variable]['scores'][int(yval)][int(xval)])\n",
    "   # test = (((specific[dependent_variable]['scores'][int(yval)][int(xval)])))\n",
    "  #  print((int(test))\n",
    "\n",
    "\n",
    "def single_var_procedure(dependent_variable, dependent_variable_value):\n",
    "    try:\n",
    "        val = find_closest(specific[dependent_variable]['names'], dependent_variable_value)\n",
    "        return(specific[dependent_variable]['scores'][int(val)])\n",
    "  #  print(val)\n",
    "    except:\n",
    "        print(\"val broke\")\n",
    "\n",
    "\n",
    "def procedure(dependent_variable, dependent_variable_value, *pairwise_y_val):\n",
    "    if pairwise_bool(dependent_variable):\n",
    "        try:\n",
    "            return pairwise_procedure(dependent_variable, dependent_variable_value, *pairwise_y_val)\n",
    "        except:\n",
    "            print(\"missing an input coordinate\")\n",
    "    else:\n",
    "        if pairwise_y_val:\n",
    "            print(\"careful either the selection if the selection is not a pairwise graph the extra coordinate does not apply\")\n",
    "        return single_var_procedure(dependent_variable, dependent_variable_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ITERATING through each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counting all Nans everywhere\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#the goal here is grab the trend across one pairwise graph for a pair of coordinates\n",
    "#specific series is an array of all the modeled datasets with outcomes\n",
    "#an issue is that there is either a null from my work or in the data. And are all nulls only at the tail?\\\n",
    "#need to figure out whether the procedure method pulls horizontally or vertically\n",
    "data_to_graph = []\n",
    "def across_datasets(graph_index, x, *y):\n",
    "    for i in specific_series:\n",
    "       # a = str(\"result0\")\n",
    "        specific = (i)\n",
    "      #  result = procedure(-1,50,51)\n",
    "      #  print(result, \"result\")\n",
    "       # data_to_graph.append(procedure(-1,50,51))\n",
    "        data_to_graph.append(procedure(graph_index,x,*y))\n",
    "        #array([-0.17544706,  0.23725504])\n",
    "    #scores': array([-0.1678648 ,  0.20165577])\n",
    "    print(data_to_graph)\n",
    "across_datasets(6,100)\n",
    "#hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show(p1.globals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for a dataset where time is a variable\n",
    "#extract the heatmap with time in the pairwise horizontally, and compare it to each time periods single variable\n",
    "#contribution value if each time period was run seperately\n",
    "panel_data\n",
    "panel_data_graph = ga2m(panel_data)\n",
    "result_panel = panel_data_graph.plot()\n",
    "specific_panel = (result_panel['specific'])\n",
    "show(panel_data_graph.globals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to reinitialize some ga2m parameters to compare the split up datasets to this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete this?\n",
    "\"\"\"\n",
    "specific = specific_panel\n",
    "print(specific[4])\n",
    "print(specific[4]['type'])\n",
    "print('\\n')\n",
    "print(pairwise_bool(4))\n",
    "#procedure(4,50)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "from statistics import median\n",
    "#print(mean(data_to_graph_split_periods))\n",
    "#print(statistics.median(data_to_graph_split_periods))\n",
    "\n",
    "data_to_graph_split_periods = data_to_graph[1:]\n",
    "data_to_graph_all_period = data_to_graph[0]\n",
    "print('combined data', data_to_graph_all_period)\n",
    "print()\n",
    "mean = (mean(data_to_graph_split_periods))\n",
    "median = (median(data_to_graph_split_periods))\n",
    "print('mean', mean)\n",
    "print('median', median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Something weird with the plot references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l1, = plt.plot(data_to_graph_split_periods) # Plot list. x-values assumed to be [0, 1, 2, 3]\n",
    "plt.grid()\n",
    "plt.xlabel('Time Period');\n",
    "plt.ylabel('Probability Contribution');\n",
    "plt.title('Probability Contribution as a Function of Time')\n",
    "l2, =  plt.plot(data_to_graph_all_period)\n",
    "plt.axhline(y=data_to_graph_all_period, color='r', linestyle='-')\n",
    "plt.axhline(y=mean, color='y', linestyle='-')\n",
    "plt.axhline(y=median, color='g', linestyle='-')\n",
    "plt.legend(['split periods', 'median', 'combined data', 'median'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(single_var_procedure(1,75))\n",
    "print(pairwise_procedure(-1,250,150))\n",
    "print(pairwise_procedure(-1,80,40))\n",
    "print(pairwise_procedure(-1,85,51))\n",
    "#print((type(specific[1]['scores'])))\n",
    "#print(specific[1]['scores'][1])\n",
    "print(procedure(-1,250,150))\n",
    "print(procedure(2,75))\n",
    "print(procedure(2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len((specific[-1]['left_names'][0:-1])))\n",
    "print(len((specific[-1]['left_names'])))\n",
    "print('\\n')\n",
    "print(len((specific[-1]['right_names'][0:-1])))\n",
    "#test = (specific[-1]['scores']).tolist()\n",
    "#print(min(test))\n",
    "print('\\n')\n",
    "print(len((specific[-1]['scores'])))\n",
    "print(len((specific[-1]['scores'][0])))\n",
    "print(len((specific[-1]['scores'][-1])))\n",
    "#are there actually 256*123 scores predefined? 31,000. That makes sense for every possible intersction of left*right\n",
    "#I think the 123 scores lists are all associated with a specific point in either the set of left names or the set of right names but not\n",
    "#both\n",
    "\n",
    "#i think it always takes from the y axis ('right names') and shows you horizontally each value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n', \"yo\")\n",
    "print(specific[1]['names'])\n",
    "print('\\n', \"yo\")\n",
    "print(specific[1]['scores'])\n",
    "print('\\n')\n",
    "print(specific[1]['scores_range'][-1])\n",
    "\n",
    "print(len(specific[1]['names']))\n",
    "print(len(specific[1]['scores']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#all of the heatmap value at the intersection of the \n",
    "#first coordinate (left,right) of each dataframe\n",
    "#Left and Right names correspond to coordinates, while scores\n",
    "#corresponds to the coefficient values computed by the model\n",
    "#for those coordinate input values from the sample\n",
    "#left is x axis\n",
    "print(specific[-1]['left_names'])\n",
    "print('\\n')\n",
    "print(specific[-1]['right_names'])\n",
    "print('\\n')\n",
    "print('scores 0', specific[-1]['scores'][0])\n",
    "print('\\n')\n",
    "print('scores 1', specific[-1]['scores'][1])\n",
    "print('\\n')\n",
    "print('scores 3', specific[-1]['scores'][2])\n",
    "print('\\n')\n",
    "print(specific[-1]['scores_range'][0])\n",
    "print('\\n')\n",
    "print(specific[-1]['scores_range'][1])\n",
    "\n",
    "\n",
    "\n",
    "#verift which pair of variables this heatmap is associated with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#example of a heatmap score for a given pair of coordinates\n",
    "[33.0, Left\n",
    " [16.59, right\n",
    "   Its 0.041\n",
    "  #and check if its a 1:1 mapping of possible coordinate intersections and the # of values in the values table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lookup table in between explicit ranges conforming\n",
    "#to the original researcher's stated rule\n",
    "#nearest defined value heuristic\n",
    "#\"for example lets say my training data has people aged \n",
    "#0-100 if you ask for a prediciton on someone who is \n",
    "#120 years old, we will simply use the same value as \n",
    "#we learned for the age 100 person\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if time is horiz axis on heatmap then vertical axis is mined for data, V.Versa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
